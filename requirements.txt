# ─────────────────────────────────────────────────────────────
# requirements.txt
# Efficient Serving & Knowledge Distillation — HPC Framework
# ─────────────────────────────────────────────────────────────
# IMPORTANT: Install with the PyTorch CUDA wheel index so that torch>=2.9.1
# resolves to a GPU-enabled build (PyPI only ships CPU wheels for torch):
#
#   pip install -r requirements.txt \
#       --extra-index-url https://download.pytorch.org/whl/cu121
#
# To force-upgrade an existing environment (e.g. if torch 2.5.x was
# previously installed by a stale manual command):
#
#   pip install --upgrade "torch>=2.9.1" \
#       --extra-index-url https://download.pytorch.org/whl/cu121
#   pip install --upgrade -r requirements.txt \
#       --extra-index-url https://download.pytorch.org/whl/cu121

# Core ML stack
# torch must be installed via the PyTorch CUDA wheel index (see header above).
# The cu121 index tops out at 2.5.1; use the cu124 index to get >=2.6.0.
# vllm 0.15.1 requires torch>=2.6.0 (libc10.so ABI incompatibility with 2.5.x).
torch>=2.6.0
transformers>=4.38
datasets>=2.16
peft>=0.8
accelerate>=0.25

# Serving
vllm>=0.15.1

# HTTP / async client
httpx>=0.25
uvicorn>=0.24
fastapi>=0.108

# Configuration & logging
pyyaml>=6.0
python-json-logger>=2.0

# Numerical
numpy>=1.24
