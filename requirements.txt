# ─────────────────────────────────────────────────────────────
# requirements.txt
# Efficient Serving & Knowledge Distillation — HPC Framework
# ─────────────────────────────────────────────────────────────
# NOTE: torch must be installed FIRST with the CUDA wheel index:
#   pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121
# Then install the rest:
#   pip install -r requirements.txt

# Core ML stack
# torch version must match vllm's exact requirement.
# vllm 0.15.1 requires torch==2.9.1. If torch is installed from PyPI without
# a CUDA wheel index the binary may lack GPU support; see README §5.2.
torch>=2.9.1
transformers>=4.38
datasets>=2.16
peft>=0.8
accelerate>=0.25

# Serving
vllm>=0.15.1

# HTTP / async client
httpx>=0.25
uvicorn>=0.24
fastapi>=0.108

# Configuration & logging
pyyaml>=6.0
python-json-logger>=2.0

# Numerical
numpy>=1.24
