# ─────────────────────────────────────────────────────────────
# requirements.txt
# Efficient Serving & Knowledge Distillation — HPC Framework
# ─────────────────────────────────────────────────────────────
# NOTE: torch must be installed FIRST with the CUDA wheel index:
#   pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121
# Then install the rest:
#   pip install -r requirements.txt

# Core ML stack
# torch is intentionally omitted here — install it via the CUDA wheel index
# (see note above) before running this file. The lower bound is listed for
# documentation purposes: vllm>=0.6 requires torch>=2.4.
torch>=2.4
transformers>=4.38
datasets>=2.16
peft>=0.8
accelerate>=0.25

# Serving
vllm>=0.6

# HTTP / async client
httpx>=0.25
uvicorn>=0.24
fastapi>=0.108

# Configuration & logging
pyyaml>=6.0
python-json-logger>=2.0

# Numerical
numpy>=1.24
