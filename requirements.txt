# ─────────────────────────────────────────────────────────────
# requirements.txt
# Efficient Serving & Knowledge Distillation — HPC Framework
# ─────────────────────────────────────────────────────────────
# IMPORTANT: Install with the PyTorch CUDA wheel index so that torch>=2.9.1
# resolves to a GPU-enabled build (PyPI only ships CPU wheels for torch):
#
#   pip install -r requirements.txt \
#       --extra-index-url https://download.pytorch.org/whl/cu121
#
# To force-upgrade an existing environment (e.g. if torch 2.5.x was
# previously installed by a stale manual command):
#
#   pip install --upgrade "torch>=2.9.1" \
#       --extra-index-url https://download.pytorch.org/whl/cu121
#   pip install --upgrade -r requirements.txt \
#       --extra-index-url https://download.pytorch.org/whl/cu121

# Core ML stack
# torch version must match vllm's exact requirement.
# vllm 0.15.1 requires torch==2.9.1. If torch is installed from PyPI without
# a CUDA wheel index the binary may lack GPU support; see README §5.2.
torch>=2.9.1
transformers>=4.38
datasets>=2.16
peft>=0.8
accelerate>=0.25

# Serving
vllm>=0.15.1

# HTTP / async client
httpx>=0.25
uvicorn>=0.24
fastapi>=0.108

# Configuration & logging
pyyaml>=6.0
python-json-logger>=2.0

# Numerical
numpy>=1.24
