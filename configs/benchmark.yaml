# ─────────────────────────────────────────────────────────────
# benchmark.yaml — Benchmarking configuration
# ─────────────────────────────────────────────────────────────

common:
  base_url: "http://localhost:8000"
  model: "Qwen/Qwen2.5-72B-Instruct"
  seed: 42
  results_base_dir: "results"

throughput:
  # Offline throughput benchmark via `vllm bench serve`
  dataset_name: "sharegpt"            # sharegpt | sonnet | random
  dataset_path: "data/ShareGPT_V3_unfiltered_cleaned_split.json"
  num_prompts: 1000
  request_rate: "inf"                 # inf = closed-loop (max throughput)
  input_len: null                     # only for random dataset
  output_len: null                    # only for random dataset
  max_concurrency: null               # null = unlimited

online:
  # Online load benchmark — async HTTP client
  prompts_file: "configs/prompts.jsonl"
  num_requests: 500
  request_rates: [1, 5, 10, 20, 50]  # requests per second
  max_tokens: 256
  temperature: 0.0
  timeout_seconds: 120
  concurrency: 64
