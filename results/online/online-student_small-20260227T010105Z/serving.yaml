# ─────────────────────────────────────────────────────────────
# serving.yaml — vLLM serving configuration
# ─────────────────────────────────────────────────────────────

server:
  host: "0.0.0.0"
  port: 8000

# Model is resolved from configs/models.yaml → teacher.name
# Override here only for one-off experiments.
vllm:
  model: "Qwen/Qwen2.5-14B-Instruct"
  tensor_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.90
  dtype: "auto"
  trust_remote_code: false
  enforce_eager: false
  swap_space: 4              # GiB of CPU swap per GPU
  max_num_seqs: 256
  seed: 42

api:
  api_key: ""                # empty = no auth
  chat_template: null        # path to Jinja2 chat template; null = model default
  response_role: "assistant"

logging:
  level: "INFO"
  format: "json"             # json | text
  log_dir: "logs"
